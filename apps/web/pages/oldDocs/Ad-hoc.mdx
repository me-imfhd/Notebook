
## Video Transcoder

[bookmark](https://slides.com/harkiratsingh-4/extra-class)


## Getting Started


---


> 💡 This is built over 2 years with a lot of trail and error by getting live on day 0 and improving it on the way. Not the usual FAANG way. 😉


**Ways to deploy**

1. **Vercel/Netlify** : A drag & drop way.
2. Auto-Scale as load increases, but on **native machines** (without using containerization) : It’s like renting a computer say AWS EC2 and running an application.
3. **Containerized**/**Dockerized** and deployed natively (without Kubernetes/terraform) : It’s like putting everything in a box and then running it.
4. Using **Terraform** (Managing cloud via code), **Kubernetes** (Docker Management Tool)

[bookmark](https://youtu.be/Gjnup-PuquQ)


[bookmark](https://youtu.be/PziYflu8cB8)


[bookmark](https://youtu.be/tomUWcQ0P3k)


**Two pipelines incurring cost** : Transcoding(60%) and Distribution(40%)


**AWS Services used**

1. **S3** : To Store the videos
2. **Elastic** : Run transcoding

## How apps are deployed


---


> 💡 Frontend & Backend are deployed separately. 


It is because **frontend** files are usually `index.html`, `script.js`, `style.css` or even if there is a framework like React, Next, Vue etc. In the end, it will spit out these files.


Frontends typically deployed in `CDN` (**Content Delivery Network**), which is a network/collection of servers present around the globe, these servers are called `POP` (**Point of Presence**), upon the request the nearest POP handles it. **Netlify/Vercel** is an example of same.


Another **popular use case** of `CDN` is delivering video content. As requesting a YouTube video hosted in the USA from India wouldn’t make sense. So the nearest POP typically server that.


> ⚠️ Do note : When we push content to a CDN, it is deployed to specific POPs rather than all of them. The CDN dynamically caches the content based on regional demand and traffic.


But **backends** can’t be deployed to CDN, because content is dynamic/different/change as per the user request. In short, CDNs are best for static content, not for dynamic content. So dedicated servers are required.


Ways to deploy

1. **Repl.it** : A “jugad”/makeshift. Will cost nothing.
2. **Serverless** : Paying as per requests. [fly.io](http://fly.io/) or Firebase Functions.
3. **AWS/GCP** : Renting a server, thus a fixed-cost will incur regardless of use.

> ⚠️ Full stacks are easy to deploy, but transcoders are usually a little complex.


## Understanding Video Apps


---


**Requirements**


1. User can upload a video up to 4 GB. 


2. Once the video is uploaded, it should be transcoded to many formats like 480p, 720p etc.


3. Students should be given the option to choose a quality to save bandwidth.


4. Should be able to handle 10000 videos/day.


5. Should do it in a cost-effective.


> 💡 Transcoding is like creating 480p, 720p etc versions of video. Like we see on YouTube.


**Architecture** (Internal Working of transcoder) : Transcoding a video takes around 30 min, performing this on main server, will make our service down, which is not an idea. Thus, it is typically performed on a separate server. This also demonstrate the need of backend talking to backends. As once the video is uploaded to the main server, it should auto inform/request the transcoder server to transcode the video. That is why, sending emails, OTPs etc. are done on separate servers.


![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd32e22-0b9d-4863-baa0-84dfdaf49ec9/c89e10e3-db0b-40b3-b76f-0c715bff3b54/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240118%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20240118T150951Z&X-Amz-Expires=3600&X-Amz-Signature=ef5c9674be54d132007708b3d7f1520cad0b4daea1249611070ef06d77193217&X-Amz-SignedHeaders=host&x-id=GetObject)


**How push and pull happens**


In a **push-based approach**, the transcoder endpoint is immediately triggered as soon as the video is received on the main server. However, there is a possibility that the transcoder server may miss the request, be busy, or even be down.


To address this, a **pull-based approach** is employed, where the transcoder server requests the video when it is available and free to do so. The main server maintains a queue of videos, ensuring a systematic flow and no loss of requests. Below we are using the pull-based approach.


![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd32e22-0b9d-4863-baa0-84dfdaf49ec9/c6e59c5a-3b7b-45ef-a941-5616156f9333/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240118%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20240118T150950Z&X-Amz-Expires=3600&X-Amz-Signature=66ed6f0306a5c558d64a53e2e4a422ffdaa8ac411d2489346e3489fc50e24b65&X-Amz-SignedHeaders=host&x-id=GetObject)


**Response from Main Server**


[bookmark](https://gist.github.com/hkirat/25392c336666a6d44d719093c8bec063)


```json
// When video requested from main server
{
   "status":200,
   "data":{
      "id":"127502",
      "stream_id":"1084588476",
      "input_url":"https://a.cloudfront.com/video.mp4",
   }
}

// Another endpoint tp check the queue length
{
   "status":200,
   "data":{
      "queue_length":"5",
   }
}
```


**Request sent to main server**


[bookmark](https://gist.github.com/hkirat/9ed75aa3fbf0c16f61a741ab1f105ea3)


```json
// When video is transcoded, this is send to main server.
{
   "status":200,
   "data":{
      "id":"127502",
      "stream_id":"1084588476",
      "input_url":"https://a.cloudfront.com/video.mp4",
      "outputUrls":{
         "drm":[
            {
               "name":"720p",
               "url":"https://a.cloudfront.com/video_id_123/720p/drm/stream.mpd"
            },
            {
               "name":"480p",
               "url":"https://a.cloudfront.com/video_id_123/480p/drm/stream.mpd"
            },
            {
               "name":"360p",
               "url":"https://a.cloudfront.com/video_id_123/360p/drm/stream.mpd"
            },
            {
               "name":"240p",
               "url":"https://a.cloudfront.com/video_id_123/240p/drm/stream.mpd"
            }
         ],
         "mp4":[
            {
               "name":"720p",
               "url":"https://a.cloudfront.com/video_id_123/720p/drm-hls/master-5000321.108390829.mp4"
            },
            {
               "name":"480p",
               "url":"https://a.cloudfront.com/video_id_123/480p/drm-hls/master-5000321.108390829.mp4"
            },
            {
               "name":"360p",
               "url":"https://a.cloudfront.com/video_id_123/360p/drm-hls/master-5000321.108390829.mp4"
            },
            {
               "name":"240p",
               "url":"https://a.cloudfront.com/video_id_123/240p/drm-hls/master-5000321.108390829.m3u8.m3u8"
            }
         ],
         "encrypted":{
            "url":"https://a.cloudfront.com/video_id_123/encrypted.mp4",
            "password":"6142136"
         },
         "duration":"3846"
      },
   }
}
```


## How to do it ?


---


In cloud, we are charged for the amount of time-server is up. So, based on the number of videos in the queue, the number of transcoding servers should auto-scale only for that amount of time. Hence, charge will be only for the time-server is up, not for provisioned/rented server.


**Using Lambda** : AWS serverless function, but ideal not for the long-running task like transcoding.


**Approach 1** : Bring up a new server for new video, transcode and kill itself.


**Approach 2** : Heuristic guess for number of severs. Saves money upon booting time but complex.


**Approach 3** : Rent minimum number of servers required all time like for 3 years.


**Approach 4** : For mp4 to mp4 conversion, we need only 1 core, but for mp4 to DRM conversion we need 32 cores. So using 32 cores servers for mp4 to mp4 conversion wouldn’t make sense. Create _separate auto-scale groups_. If in future we have 10 types, then 10 groups wouldn’t make sense.


**Approach 5** : Containerization, Auto scale and pay per core via Kubernetes (_Cluster will go up upon CPU usage_). The other way is Auto-Scaling group policy, but that may cause random killing.


**Approach 6** : GPU at Home as it is not user facing all time.


## Advance


---


[bookmark](https://youtu.be/NQ3fZtyXji0)


[bookmark](https://youtu.be/uvb00oaa3k8)


**Prometheus** : K8s Monitoring Tool


**Libraries for transcoding** : Google FFmpeg, GStreamer


**Spot Instance** : Normally, you would pay a fixed price for the computer on an hourly basis. But with EC2 Spot Instances, you have the option to bid for a lower price. Its like trading of EC2


## G-Meet Clone

[link_preview](https://github.com/100xDevs-hkirat/gmeet-webrtc)


[bookmark](https://slides.com/harkiratsingh-6/building-gmeet)


## Introduction


---

1. **Going niche is different but better** : Separate from crowd.
2. WebRTC Story
3. G-Meet (Hard Parts)

**Right Hot Niche Tech**

1. Look for good paying & in demand niche.
2. Like today, **AI** (The Best) ✅, **Web3** (Later) ✅, WebRTC ❌
3. Then **Learning** 3 Months, **Underpricing** 3 Months, **Work Experience** 3 Months, **Ask your Bid** 3 Months.

> 😮 8–10 people company Streamyard got acquired for $250M


**Freelancing Path**

1. **First client** – Charged $30/hour – 6 months – Used [Agora](https://www.agora.io/en/), [LiveKit](https://livekit.io/)
2. **Second client** – Charged $70k – $100k, exit at $150k – 1 Year – Built
the in house streaming from scratch, Real WebRTC Learning/Expert
3. **Third company** – $500k Offer ($200k Cash, rest stock) – Joined as the video lead – 1 Year

## Learning WebRTC


---


Achieving Real Time Communication

1. **Polling/Jugaad** : Sending request after a specific interval.
2. **Websockets/SSE** (Server Side Events) : Server automatically sends the updates whenever data is added/udpated. Basically whenever an event happens it is sent to frontend.

> 💡 Like we built **HTTP Server** and used HTTP protocol, similarly we can build **WebSocket Server**, using WebSocket Protocol.


## Web Socket Demo


---


[Socket.io](https://socket.io/)


```javascript
npm install express@4 socket.io
```


```javascript
**// Index.js**

const app = require('express')();
const http = require('http').llServer(app);
const io = require('socket.io')(http);
const port = process.env.PORT || 3000;

app.get('/', (req, res) => {
  res.sendFile(__dirname + '/index.html');
});

io.on('connection', (socket) => {
  socket.on('chat message', msg => {
    io.emit('chat message', msg);
  });
});

http.listen(port, () => {
  console.log(`Socket.IO server running at http://localhost:${port}/`);
});
```


> 💡 So with WebRTC, Browser can directly talk to browser(**P2P**), without use of server.


```javascript
**// Index.html**

<!DOCTYPE html>
<html>
  <head>
    <title>Socket.IO chat</title>
    <style>
      body { margin: 0; padding-bottom: 3rem; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; }

      #form { background: rgba(0, 0, 0, 0.15); padding: 0.25rem; position: fixed; bottom: 0; left: 0; right: 0; display: flex; height: 3rem; box-sizing: border-box; backdrop-filter: blur(10px); }
      #input { border: none; padding: 0 1rem; flex-grow: 1; border-radius: 2rem; margin: 0.25rem; }
      #input:focus { outline: none; }
      #form > button { background: #333; border: none; padding: 0 1rem; margin: 0.25rem; border-radius: 3px; outline: none; color: #fff; }

      #messages { list-style-type: none; margin: 0; padding: 0; }
      #messages > li { padding: 0.5rem 1rem; }
      #messages > li:nth-child(odd) { background: #efefef; }
    </style>
  </head>
  <body>
    <ul id="messages"></ul>
    <form id="form" action="">
      <input id="input" autocomplete="off" /><button>Send</button>
    </form>
    <script src="/socket.io/socket.io.js"></script>

    <script>
      var socket = io();

      var messages = document.getElementById('messages');
      var form = document.getElementById('form');
      var input = document.getElementById('input');

      form.addEventListener('submit', function(e) {
        e.preventDefault();
        if (input.value) {
          socket.emit('chat message', input.value);
          input.value = '';
        }
      });

      socket.on('chat message', function(msg) {
        var item = document.createElement('li');
        item.textContent = msg;
        messages.appendChild(item);
        window.scrollTo(0, document.body.scrollHeight);
      });
    </script>
  </body>
</html>
```


> ⚠️ **Web Socket** internally uses **TCP** (It makes sure connection is established, like in payments, a connection is must), while WebRTC uses **UDP** (It doesn’t care about connection, just sends the data).


## WebRTC


---


No URLs here like we used in fetch or axios, instead **STUN Server** is used to get IP + PORT of the client, which is later shared between two to establish the connection.


**Signalling Sever** : Server in middle to share the IP + PORT between clients.


![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd32e22-0b9d-4863-baa0-84dfdaf49ec9/2839f86a-860a-45af-9c10-3bab799a361d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240118%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20240118T150952Z&X-Amz-Expires=3600&X-Amz-Signature=6aace9e90d1d7d3a84b907d77bd1891f6e50e5c75d14a10781b7b29086ab65b2&X-Amz-SignedHeaders=host&x-id=GetObject)


**Ice Candidates** : PROTOCOL + IP + PORT makes an Ice Candidate, like `udp 122.172.82.7130700`


[Ice Candidate Test](https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/)


**Peer Connection Object** : `RTCPeerConnection` a global object in browser.


![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd32e22-0b9d-4863-baa0-84dfdaf49ec9/b33d2d56-a1f6-4ebe-a8b7-c7a4b6bb7af7/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240118%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20240118T150952Z&X-Amz-Expires=3600&X-Amz-Signature=716a2cedb3627241374d264b63ca5ab686a969a07d3f4377b95b7ea23f830393&X-Amz-SignedHeaders=host&x-id=GetObject)


**localDescription** : One of the property when we create a new object of RTCPeerConnection. This holds the data like IP, PORT. Simply the information that need to be shared.


![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd32e22-0b9d-4863-baa0-84dfdaf49ec9/3c619268-aaa8-489d-bde9-9178adfcc022/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240118%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20240118T150952Z&X-Amz-Expires=3600&X-Amz-Signature=5f487831ebf4b10d5c47822f43a56a64ae93cdc631f9ba912428bade9941745f&X-Amz-SignedHeaders=host&x-id=GetObject)


**remoteDescription** : This property holds the data about the another browser/peer with whom we can to establish the connection.


![Untitled.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/4dd32e22-0b9d-4863-baa0-84dfdaf49ec9/c324f6e9-23c4-4445-a13d-41d650c597e1/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45HZZMZUHI%2F20240118%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20240118T150952Z&X-Amz-Expires=3600&X-Amz-Signature=ae7d35deb1f31c35b9bd759190358e66f1ef10bf7bfd00d74185dec368b16df2&X-Amz-SignedHeaders=host&x-id=GetObject)


> 💡 Think of this as  
> `localDescription` = Browser 1 IP, PORT etc.  
> `remoteDescription` = Browser 2 IP, PORT etc.


**SDP** (Session Description Protocol) : Exchanging the local + remote description between two browsers.


**Tracks** : Once SDP is done, `addTrack()` to sends video track, `onTrack()` to receive the video track.


🚀 The below demo is doing the same in the same browser, without need of signalling server.


[bookmark](https://jsfiddle.net/rainzhao/3L9sfsvf/)


**Check the connection** : `chrome://webrtc-internals/`


> 😂 Sucks : `navigator.mediaDevices.getUserMedia({video:true})` only line to turn on video.

